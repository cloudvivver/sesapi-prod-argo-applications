apiVersion: v1
data:
  # Airflow Core Configuration
  AIRFLOW__CORE__EXECUTOR: "KubernetesExecutor"
  AIRFLOW__CORE__AUTH_MANAGER: "airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
  AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  AIRFLOW__CORE__FERNET_KEY: ""

  # Database Configuration
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@postgres:5432/airflow"

  # Celery Configuration
  AIRFLOW__CELERY__RESULT_BACKEND: "db+postgresql://airflow:airflow@postgres:5432/airflow"
  AIRFLOW__CELERY__BROKER_URL: "redis://:@redis:6379/0"

  # Webserver Configuration
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
  AIRFLOW__WEBSERVER__SECRET_KEY: "your-secret-key-here"

  # Scheduler Configuration
  AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"

  # Redis Configuration
  REDIS_HOST: "redis"
  REDIS_PORT: "6379"
  REDIS_DB: "0"

  # Performance Configuration
  AIRFLOW__CORE__PARALLELISM: "32"
  AIRFLOW__CORE__DAG_CONCURRENCY: "16"
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "16"

  # Logging Configuration
  AIRFLOW__LOGGING__LOGGING_LEVEL: "INFO"
  AIRFLOW__LOGGING__REMOTE_LOGGING: "false"

  # Environment
  PYTHONPATH: "/opt/airflow/dags:/opt/airflow/plugins"

  # Kubernetes Executor Configuration
  AIRFLOW__KUBERNETES__NAMESPACE: "saude-airflow"
  AIRFLOW__KUBERNETES__WORKER_CONTAINER_REPOSITORY: "apache/airflow"
  AIRFLOW__KUBERNETES__WORKER_CONTAINER_TAG: "3.0.6"
  AIRFLOW__KUBERNETES__DELETE_WORKER_PODS: "true"
  AIRFLOW__KUBERNETES__DELETE_WORKER_PODS_ON_FAILURE: "false"
  AIRFLOW__KUBERNETES__WORKER_PODS_CREATION_BATCH_SIZE: "5"

kind: ConfigMap
metadata:
  labels:
    io.kompose.service: saude-airflow-env
  name: env